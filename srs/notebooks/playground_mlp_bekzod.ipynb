{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a88072e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3e446e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% load packages\n",
    "import locale\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import requests\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import random\n",
    "from sqlalchemy import create_engine,inspect\n",
    "from pathlib import Path\n",
    "import urllib.parse\n",
    "import pyarrow\n",
    "from calendar import day_abbr\n",
    "import calendar\n",
    "from typing import Tuple, Union, Dict, List\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pygam import LinearGAM\n",
    "from datetime import datetime\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22aca3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from srs.utils.tutor_utils import prepare_dataset_tensor, forecasting_study,\\\n",
    "  plot_daily_profile,plot_hour_comparison, build_multiwindow_experts, tune_ewa_eta, \\\n",
    "  ewa_aggregate_forecasts, compute_error_table, tune_expert_window, \\\n",
    "  run_expert_window_test, build_regression_matrix, SimpleMLP, train_mlp, \\\n",
    "  prepare_train_test_tensors, build_mlp_rolling_forecasts, tune_mlp_hyperparameters, \\\n",
    "  DST_trafo, prepare_dataset_tensor_modified\n",
    "\n",
    "from srs.utils.our_utils import run_forecast_step\n",
    "from srs.collect_data.setup import setup_seed, get_device\n",
    "from srs.collect_data.entsoe_data import create_entsoe_engine, get_tables, get_spec, \\\n",
    "  get_market_divisions,get_map_codes,get_map_codes_starting_with, get_resolution_codes, \\\n",
    "    prepare_generation, prepare_load,prepare_price, prepare_unavailability, \\\n",
    "    prepare_filling_rate_hydro, prepare_physical_flow, prepare_installed_capacity\n",
    "from srs.collect_data.datastream_data import create_datastream_engine, get_tables, \\\n",
    "  prepare_datastream\n",
    "from srs.collect_data.dwd_mosmix_data import fetch_region_weather, prepare_weather\n",
    "from srs.collect_data.merge_data import merge_datasets, build_training_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b65b8137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_t shape: torch.Size([2193, 24, 10])\n",
      "train_t shape: torch.Size([1463, 24, 10])\n",
      "train_dates shape: (1463,)\n",
      "price_t shape: torch.Size([2193, 24])\n"
     ]
    }
   ],
   "source": [
    "print(f\"data_t shape: {data_t.shape}\")\n",
    "print(f\"train_t shape: {train_t.shape}\")\n",
    "print(f\"train_dates shape: {train_dates.shape}\")\n",
    "print(f\"price_t shape: {price_t.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3981770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['regmat', 'index_dict', 'dep_indices'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "abc74e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 8,\n",
       " 16,\n",
       " 24,\n",
       " 32,\n",
       " 40,\n",
       " 48,\n",
       " 56,\n",
       " 64,\n",
       " 72,\n",
       " 80,\n",
       " 88,\n",
       " 96,\n",
       " 104,\n",
       " 112,\n",
       " 120,\n",
       " 128,\n",
       " 136,\n",
       " 144,\n",
       " 152,\n",
       " 160,\n",
       " 168,\n",
       " 176,\n",
       " 184]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reg_data[\"dep_indices\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fc071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  training interval:\n",
    "  2019 - 365 days\n",
    "  2020 - 366 days\n",
    "  2021 - 365 days\n",
    "  2022 - 366 days\n",
    "  \n",
    "  testing interval:\n",
    "  2023 - 365 days\n",
    "  2024 - 366 days\n",
    "  \n",
    "  \n",
    "  The reason why I have metnioned lightgbm and GAM before is that I have, as one of the alternative methodologies, to get preliminary predictions from gam,lightbgm or any other models, use these predictions as a input for a input layer to get final predictions from MLP.\n",
    "That is why I need to be consistent for now with \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66047abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# **************************************\n",
    "# define dates for training and evaluation \n",
    "# **************************************\n",
    "INIT_DATE_EXPERIMENTS = '2019-01-01'\n",
    "INIT_TEST_DATE = '2023-01-01'\n",
    "FINAL_DATE_EXPERIMENTS = '2024-12-31'\n",
    "n_days_test = (pd.to_datetime(FINAL_DATE_EXPERIMENTS) - pd.to_datetime(INIT_TEST_DATE)).days + (1) # additional adjustment\n",
    "\n",
    "repo_root = Path.cwd().parents[1]\n",
    "mapcodes = [\"NO1\",\"NO2\",\"NO3\",\"NO4\",\"NO5\"]\n",
    "maps_dict = {}\n",
    "\n",
    "for code in mapcodes:\n",
    "    csv_path = repo_root / \"data\" / f\"data_{code}.csv\"\n",
    "    df = pd.read_csv(csv_path, parse_dates=[\"time_utc\"])\n",
    "    data_t, train_t, train_dates, price_t = prepare_dataset_tensor( # <- update function to Alex's one\n",
    "        csv_path,\n",
    "        tz=\"CET\",\n",
    "        seed=42,\n",
    "        test_days=n_days_test,\n",
    "        dtype=torch.float64,\n",
    "    )\n",
    "    \n",
    "    # fix potential problems with dates after change time zone.. (Alex correction)\n",
    "    train_dates_series       = pd.DatetimeIndex(sorted(train_dates))\n",
    "    id_init_exp = train_dates_series.get_loc(pd.Timestamp(INIT_DATE_EXPERIMENTS))\n",
    "    id_init_test_period = train_dates_series.get_loc(pd.Timestamp(INIT_TEST_DATE))\n",
    "    id_end_exp = train_dates_series.get_loc(pd.Timestamp(FINAL_DATE_EXPERIMENTS))\n",
    "    data_t = data_t[id_init_exp:(id_end_exp+1), :,:]\n",
    "    train_dates = pd.Series(train_dates[id_init_exp:(id_end_exp+1)])\n",
    "    \n",
    "    maps_dict[code] = {\n",
    "        \"df\": df,\n",
    "        \"data_t\": data_t,\n",
    "        \"train_t\": train_t,\n",
    "        \"train_dates\": train_dates,\n",
    "        \"price_t\": price_t\n",
    "    }\n",
    "maps_dict.keys()\n",
    "\n",
    "gam24_by_zone = {}\n",
    "rmse_by_zone  = {}\n",
    "\n",
    "for z in mapcodes:\n",
    "    print(f\"\\n--- {z} ---\")\n",
    "    price_S         = maps_dict[z][\"price_t\"]\n",
    "    data_array      = maps_dict[z][\"data_t\"]\n",
    "    full_dates      = maps_dict[z][\"train_dates\"] # <- changed from _all_ days to train_dates based on Alex spot\n",
    "    feature_names   = maps_dict[z][\"df\"].columns[1:]\n",
    "    full_date_series= pd.DatetimeIndex(sorted(full_dates)) \n",
    "\n",
    "    # evaluation days (all of 2024)\n",
    "    train_start_idx = full_date_series.get_loc(pd.Timestamp(INIT_DATE_EXPERIMENTS))\n",
    "    id_init_eval = full_date_series.get_loc(pd.Timestamp(INIT_TEST_DATE))\n",
    "    id_end_eval = full_date_series.get_loc(pd.Timestamp(FINAL_DATE_EXPERIMENTS))\n",
    "    eval_start_idx = id_init_eval \n",
    "    eval_end_idx  = id_end_eval\n",
    "    N_s = (eval_end_idx - eval_start_idx) + 1\n",
    "    full_dates = pd.to_datetime(full_dates)\n",
    "    \n",
    "    # new features: WD - dummy for week days, price lags for Mon, Tue and Fri, day-ahead load lag\n",
    "    WD             = [1,6,7]     \n",
    "    PRICE_S_LAGS   = [1,2,7]\n",
    "    DA_LAG         = [0]\n",
    "    S              = 24\n",
    "    #D             = 730\n",
    "\n",
    "    # tensors to collect forecasts for THIS zone\n",
    "    forecasts_zone = torch.full((N_s, S, 1), float(\"nan\"),\n",
    "                                dtype=torch.float64, device=device)\n",
    "                \n",
    "    #   shape: (N_s, S)\n",
    "    true_vals = price_S[eval_start_idx : eval_end_idx + 1].to(device)  \n",
    "    \n",
    "    # compute RMSE\n",
    "    diff = forecasts_zone[:, :, 0] - true_vals\n",
    "    rmse = torch.sqrt((diff**2).mean()).item()\n",
    "    \n",
    "    print(range(N_s))\n",
    "    print(f\"Zone {z} GAM-24h RMSE: {rmse:.4f}\")\n",
    "\n",
    "    gam24_by_zone[z] = forecasts_zone[:, :, 0].cpu()\n",
    "    rmse_by_zone[z]  = rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b112a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Tutorial 4. Reg Matrix and simple MLP Benchmark\n",
    "\n",
    "# 7. Transform merged dataset using DST_trafo and prepare training data.\n",
    "data_t, train_t, train_dates, price_t = prepare_dataset_tensor(\n",
    "    \"./data/data_no1.csv\",\n",
    "    tz=\"CET\",\n",
    "    seed=42,\n",
    "    test_days=2*365,         \n",
    "    dtype=torch.float64, \n",
    ")\n",
    "print(train_t.shape, price_t.shape)\n",
    "\n",
    "# 1) Build regression matrix for the evaluation block\n",
    "reg_data = build_regression_matrix(\n",
    "    dat_eval = train_t.cpu().numpy(),\n",
    "    days_eval= pd.to_datetime(train_dates),\n",
    "    reg_names= df.columns[1:],   # all columns except time_utc\n",
    ")\n",
    "\n",
    "# 2) Prepare tensors for the FIRST evaluation day\n",
    "tensors = prepare_train_test_tensors(\n",
    "    regmat_df   = reg_data[\"regmat\"],\n",
    "    dep_indices = reg_data[\"dep_indices\"],\n",
    "    D           = 730,                      # window\n",
    "    eval_start_row = reg_data[\"regmat\"].shape[0] - 730,   # begin_eval\n",
    "    device      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    ")\n",
    "\n",
    "# 3) Train the MLP and get prediction vs. true\n",
    "pred, true = train_mlp(\n",
    "    tensors,\n",
    "    hidden_dim   = 50,\n",
    "    lr           = 0.001,\n",
    "    weight_decay = 0.001,\n",
    "    batch_size   = 32,\n",
    "    epochs       = 60,\n",
    ")\n",
    "\n",
    "print(\"Predicted price (all 24 series) :\", pred.numpy())\n",
    "print(\"True price       (all 24 series) :\", true.numpy())\n",
    "\n",
    "# scalar metrics for that single-day, 24-hour vector\n",
    "rmse = torch.sqrt(((pred - true) ** 2).mean()).item()\n",
    "mae  = (pred - true).abs().mean().item()\n",
    "print(f\"MLP 1-day RMSE: {rmse:.3f}  MAE: {mae:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Tutorial 5. MLP  â€” rolling-window expert + Optuna tuning\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 0) Regression matrix on *all* data (no NaNs)\n",
    "reg_data = build_regression_matrix(\n",
    "    dat_eval = train_t.cpu().numpy(),\n",
    "    days_eval= pd.to_datetime(train_dates),\n",
    "    reg_names= df.columns[1:],   # all columns except time_utc\n",
    ")\n",
    "reg_df   = reg_data[\"regmat\"].dropna().reset_index(drop=True)\n",
    "dep_idx  = reg_data[\"dep_indices\"]\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1)  Optuna tuning on a 730-day evaluation block\n",
    "eval_start  = reg_df.shape[0] - 730          # first eval row\n",
    "best_params, study = tune_mlp_hyperparameters(\n",
    "    reg_df, dep_idx, eval_window = (eval_start, 730), n_trials = 40\n",
    ")\n",
    "print(\"best params:\", best_params)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2)  Build rolling forecasts on a 730-day *test* block\n",
    "test_horizon = 730\n",
    "test_start   = reg_df.shape[0] - test_horizon\n",
    "preds_mlp, trues_mlp = build_mlp_rolling_forecasts(\n",
    "    reg_df, dep_idx,\n",
    "    window      = best_params[\"D\"],\n",
    "    horizon     = test_horizon,\n",
    "    start_row   = test_start,\n",
    "    hidden_dim  = best_params[\"hidden\"],\n",
    "    lr          = best_params[\"lr\"],\n",
    "    weight_decay= best_params[\"wd\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958edb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 3)  Append to forecast_all and compute error table\n",
    "mlp_chan  = preds_mlp.unsqueeze(2)            # (N,24,1)\n",
    "forecast_all = torch.cat([forecast_all, mlp_chan], dim=2)\n",
    "model_names  = model_names + [\"MLP\"]\n",
    "\n",
    "err_table_with_mlp = compute_error_table(forecast_all, model_names)\n",
    "print(err_table_with_mlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b5ba99",
   "metadata": {},
   "source": [
    "### MLP with rolling window and without Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99646930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global constants to all zones\n",
    "INIT_DATE_EXPERIMENTS = '2019-01-01'\n",
    "INIT_TEST_DATE        = '2023-01-01'\n",
    "FINAL_DATE_EXPERIMENTS= '2024-12-31'\n",
    "\n",
    "# hyperparameters\n",
    "WINDOW_DAYS   = 730                 \n",
    "HIDDEN_DIM    = 50                  \n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY  = 1e-3\n",
    "EPOCHS        = 60\n",
    "BATCH_SIZE    = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "repo_root  = Path.cwd().parents[1]\n",
    "mapcodes   = [\"NO1\", \"NO2\", \"NO3\", \"NO4\", \"NO5\"]\n",
    "zone_data  = {}          \n",
    "\n",
    "for code in mapcodes:\n",
    "    csv_path = repo_root / \"data\" / f\"data_{code}.csv\"\n",
    "    df_raw   = pd.read_csv(csv_path, parse_dates=[\"time_utc\"])\n",
    "\n",
    "    data_t, train_t, train_dates, price_t = prepare_dataset_tensor_modified(\n",
    "        csv_path,\n",
    "        tz      = \"CET\",\n",
    "        seed    = 42,\n",
    "        test_days = (pd.Timestamp(FINAL_DATE_EXPERIMENTS)\n",
    "                     - pd.Timestamp(INIT_TEST_DATE)).days + 1,\n",
    "        dtype   = torch.float64,\n",
    "    )\n",
    "\n",
    "    idx = pd.DatetimeIndex(sorted(train_dates))\n",
    "    start_i = idx.get_loc(pd.Timestamp(INIT_DATE_EXPERIMENTS))\n",
    "    end_i   = idx.get_loc(pd.Timestamp(FINAL_DATE_EXPERIMENTS))\n",
    "    data_t  = data_t[start_i:end_i+1]             \n",
    "    dates_t = pd.Series(train_dates[start_i:end_i+1])\n",
    "\n",
    "    zone_data[code] = dict(\n",
    "        df        = df_raw,\n",
    "        tensor    = data_t,\n",
    "        dates     = dates_t,\n",
    "        price_t   = price_t[start_i:end_i+1],\n",
    "    )\n",
    "\n",
    "# rolling-window MLP per zone\n",
    "rmse_mlp_by_zone   = {}\n",
    "preds_mlp_by_zone  = {}\n",
    "\n",
    "for code in mapcodes:\n",
    "    print(f\"\\n==== Zone {code} ====\")\n",
    "\n",
    "    reg_data = build_regression_matrix(\n",
    "        dat_eval = zone_data[code][\"tensor\"].cpu().numpy(),\n",
    "        days_eval= pd.to_datetime(zone_data[code][\"dates\"]),\n",
    "        reg_names= zone_data[code][\"df\"].columns[1:],   \n",
    "    )\n",
    "    reg_df   = reg_data[\"regmat\"].dropna().reset_index(drop=True)\n",
    "    dep_idx  = reg_data[\"dep_indices\"]\n",
    "\n",
    "    all_dates = pd.DatetimeIndex(sorted(zone_data[code][\"dates\"]\\\n",
    "                                        .iloc[len(zone_data[code][\"dates\"])\n",
    "                                             - len(reg_df):]))\n",
    "    test_start_row = all_dates.get_loc(pd.Timestamp(INIT_TEST_DATE))\n",
    "    test_end_row   = all_dates.get_loc(pd.Timestamp(FINAL_DATE_EXPERIMENTS))\n",
    "    horizon        = test_end_row - test_start_row + 1      \n",
    "\n",
    "    preds, trues = build_mlp_rolling_forecasts(\n",
    "        regmat_df   = reg_df.astype(\"float32\"),\n",
    "        dep_indices = dep_idx,\n",
    "        window      = WINDOW_DAYS,\n",
    "        horizon     = horizon,\n",
    "        start_row   = test_start_row,\n",
    "        hidden_dim  = HIDDEN_DIM,\n",
    "        lr          = LEARNING_RATE,\n",
    "        weight_decay= WEIGHT_DECAY,\n",
    "        batch_size  = BATCH_SIZE,\n",
    "        epochs      = EPOCHS,\n",
    "        device      = device,\n",
    "    )\n",
    "\n",
    "    rmse = torch.sqrt(((preds - trues) ** 2).mean()).item()\n",
    "    rmse_mlp_by_zone[code]  = rmse\n",
    "    preds_mlp_by_zone[code] = preds                   \n",
    "\n",
    "    print(f\"RMSE 2023-24: {rmse:7.3f}\")\n",
    "\n",
    "print(\"\\n===== Rolling-MLP RMSE ( NOK / MWh ) =====\")\n",
    "for z, r in rmse_mlp_by_zone.items():\n",
    "    print(f\"{z}:  {r:7.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05060eef",
   "metadata": {},
   "source": [
    "==== Zone NO1 ====\n",
    "RMSE 2023-24:  21.057\n",
    "\n",
    "==== Zone NO2 ====\n",
    "RMSE 2023-24:  24.377\n",
    "\n",
    "==== Zone NO3 ====\n",
    "RMSE 2023-24:  15.700\n",
    "\n",
    "==== Zone NO4 ====\n",
    "RMSE 2023-24:  11.912\n",
    "\n",
    "==== Zone NO5 ====\n",
    "RMSE 2023-24:  17.403"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46682343",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-energy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
