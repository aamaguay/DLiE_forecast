{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a88072e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3e446e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dl-energy-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %% load packages\n",
    "import locale\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import requests\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import random\n",
    "from sqlalchemy import create_engine,inspect\n",
    "from pathlib import Path\n",
    "import urllib.parse\n",
    "import pyarrow\n",
    "from calendar import day_abbr\n",
    "import calendar\n",
    "from typing import Tuple, Union, Dict, List\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pygam import LinearGAM\n",
    "from datetime import datetime\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22aca3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from srs.utils.tutor_utils import prepare_dataset_tensor, forecasting_study,\\\n",
    "  plot_daily_profile,plot_hour_comparison, build_multiwindow_experts, tune_ewa_eta, \\\n",
    "  ewa_aggregate_forecasts, compute_error_table, tune_expert_window, \\\n",
    "  run_expert_window_test, build_regression_matrix, prepare_train_test_tensors, \\\n",
    "  DST_trafo, prepare_dataset_tensor_modified\n",
    "\n",
    "from srs.utils.our_utils import run_forecast_step\n",
    "from srs.collect_data.setup import setup_seed, get_device\n",
    "from srs.collect_data.entsoe_data import create_entsoe_engine, get_tables, get_spec, \\\n",
    "  get_market_divisions,get_map_codes,get_map_codes_starting_with, get_resolution_codes, \\\n",
    "    prepare_generation, prepare_load,prepare_price, prepare_unavailability, \\\n",
    "    prepare_filling_rate_hydro, prepare_physical_flow, prepare_installed_capacity\n",
    "from srs.collect_data.datastream_data import create_datastream_engine, get_tables, \\\n",
    "  prepare_datastream\n",
    "from srs.collect_data.dwd_mosmix_data import fetch_region_weather, prepare_weather\n",
    "from srs.collect_data.merge_data import merge_datasets, build_training_dataset\n",
    "from srs.models.mlp import SimpleMLP, train_mlp, build_mlp_rolling_forecasts, \\\n",
    "  tune_mlp_hyperparameters, DeepMLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fc071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  training interval:\n",
    "  2019 - 365 days\n",
    "  2020 - 366 days\n",
    "  2021 - 365 days\n",
    "  2022 - 366 days\n",
    "  \n",
    "  testing interval:\n",
    "  2023 - 365 days\n",
    "  2024 - 366 days\n",
    "  \n",
    "  \n",
    "The reason why I have metnioned lightgbm and GAM before is that I have, as one of the alternative methodologies, to get preliminary predictions from gam,lightbgm or any other models, use these predictions as a input for a input layer to get final predictions from MLP.\n",
    "That is why I need to be consistent for now with \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Tutorial 5. MLP  — rolling-window expert + Optuna tuning\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 0) Regression matrix on *all* data (no NaNs)\n",
    "reg_data = build_regression_matrix(\n",
    "    dat_eval = train_t.cpu().numpy(),\n",
    "    days_eval= pd.to_datetime(train_dates),\n",
    "    reg_names= df.columns[1:],   # all columns except time_utc\n",
    ")\n",
    "reg_df   = reg_data[\"regmat\"].dropna().reset_index(drop=True)\n",
    "dep_idx  = reg_data[\"dep_indices\"]\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1)  Optuna tuning on a 730-day evaluation block\n",
    "eval_start  = reg_df.shape[0] - 730          # first eval row\n",
    "best_params, study = tune_mlp_hyperparameters(\n",
    "    reg_df, dep_idx, eval_window = (eval_start, 730), n_trials = 40\n",
    ")\n",
    "print(\"best params:\", best_params)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2)  Build rolling forecasts on a 730-day *test* block\n",
    "test_horizon = 730\n",
    "test_start   = reg_df.shape[0] - test_horizon\n",
    "preds_mlp, trues_mlp = build_mlp_rolling_forecasts(\n",
    "    reg_df, dep_idx,\n",
    "    window      = best_params[\"D\"],\n",
    "    horizon     = test_horizon,\n",
    "    start_row   = test_start,\n",
    "    hidden_dim  = best_params[\"hidden\"],\n",
    "    lr          = best_params[\"lr\"],\n",
    "    weight_decay= best_params[\"wd\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b5ba99",
   "metadata": {},
   "source": [
    "### MLP with rolling window and without Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99646930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global constants to all zones\n",
    "INIT_DATE_EXPERIMENTS = '2019-01-01'\n",
    "INIT_TEST_DATE        = '2023-01-01'\n",
    "FINAL_DATE_EXPERIMENTS= '2024-12-31'\n",
    "\n",
    "# hyperparameters\n",
    "WINDOW_DAYS   = 730                 \n",
    "HIDDEN_DIM    = 50                  \n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY  = 1e-3\n",
    "EPOCHS        = 60\n",
    "BATCH_SIZE    = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "repo_root  = Path.cwd().parents[1]\n",
    "mapcodes   = [\"NO1\", \"NO2\", \"NO3\", \"NO4\", \"NO5\"]\n",
    "zone_data  = {}          \n",
    "\n",
    "for code in mapcodes:\n",
    "    csv_path = repo_root / \"data\" / f\"data_{code}.csv\"\n",
    "    df_raw   = pd.read_csv(csv_path, parse_dates=[\"time_utc\"])\n",
    "\n",
    "    data_t, train_t, train_dates, price_t = prepare_dataset_tensor_modified(\n",
    "        csv_path,\n",
    "        tz      = \"CET\",\n",
    "        seed    = 42,\n",
    "        test_days = (pd.Timestamp(FINAL_DATE_EXPERIMENTS)\n",
    "                     - pd.Timestamp(INIT_TEST_DATE)).days + 1,\n",
    "        dtype   = torch.float64,\n",
    "    )\n",
    "\n",
    "    idx = pd.DatetimeIndex(sorted(train_dates))\n",
    "    start_i = idx.get_loc(pd.Timestamp(INIT_DATE_EXPERIMENTS))\n",
    "    end_i   = idx.get_loc(pd.Timestamp(FINAL_DATE_EXPERIMENTS))\n",
    "    data_t  = data_t[start_i:end_i+1]             \n",
    "    dates_t = pd.Series(train_dates[start_i:end_i+1])\n",
    "\n",
    "    zone_data[code] = dict(\n",
    "        df        = df_raw,\n",
    "        tensor    = data_t,\n",
    "        dates     = dates_t,\n",
    "        price_t   = price_t[start_i:end_i+1],\n",
    "    )\n",
    "\n",
    "# rolling-window MLP per zone\n",
    "rmse_mlp_by_zone   = {}\n",
    "preds_mlp_by_zone  = {}\n",
    "\n",
    "for code in mapcodes:\n",
    "    print(f\"\\n==== Zone {code} ====\")\n",
    "\n",
    "    reg_data = build_regression_matrix(\n",
    "        dat_eval = zone_data[code][\"tensor\"].cpu().numpy(),\n",
    "        days_eval= pd.to_datetime(zone_data[code][\"dates\"]),\n",
    "        reg_names= zone_data[code][\"df\"].columns[1:],   \n",
    "    )\n",
    "    reg_df   = reg_data[\"regmat\"].dropna().reset_index(drop=True)\n",
    "    dep_idx  = reg_data[\"dep_indices\"]\n",
    "\n",
    "    all_dates = pd.DatetimeIndex(sorted(zone_data[code][\"dates\"]\\\n",
    "                                        .iloc[len(zone_data[code][\"dates\"])\n",
    "                                             - len(reg_df):]))\n",
    "    test_start_row = all_dates.get_loc(pd.Timestamp(INIT_TEST_DATE))\n",
    "    test_end_row   = all_dates.get_loc(pd.Timestamp(FINAL_DATE_EXPERIMENTS))\n",
    "    horizon        = test_end_row - test_start_row + 1      \n",
    "\n",
    "    preds, trues = build_mlp_rolling_forecasts(\n",
    "        regmat_df   = reg_df.astype(\"float32\"),\n",
    "        dep_indices = dep_idx,\n",
    "        window      = WINDOW_DAYS,\n",
    "        horizon     = horizon,\n",
    "        start_row   = test_start_row,\n",
    "        hidden_dim  = HIDDEN_DIM,\n",
    "        lr          = LEARNING_RATE,\n",
    "        weight_decay= WEIGHT_DECAY,\n",
    "        batch_size  = BATCH_SIZE,\n",
    "        epochs      = EPOCHS,\n",
    "        device      = device,\n",
    "    )\n",
    "\n",
    "    rmse = torch.sqrt(((preds - trues) ** 2).mean()).item()\n",
    "    rmse_mlp_by_zone[code]  = rmse\n",
    "    preds_mlp_by_zone[code] = preds                   \n",
    "\n",
    "    print(f\"RMSE 2023-24: {rmse:7.3f}\")\n",
    "\n",
    "print(\"\\n===== Rolling-MLP RMSE ( NOK / MWh ) =====\")\n",
    "for z, r in rmse_mlp_by_zone.items():\n",
    "    print(f\"{z}:  {r:7.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082dd2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 3)  Append to forecast_all and compute error table\n",
    "mlp_chan  = preds_mlp.unsqueeze(2)            # (N,24,1)\n",
    "forecast_all = torch.cat([forecast_all, mlp_chan], dim=2)\n",
    "model_names  = model_names + [\"MLP\"]\n",
    "\n",
    "err_table_with_mlp = compute_error_table(forecast_all, model_names)\n",
    "print(err_table_with_mlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05060eef",
   "metadata": {},
   "source": [
    "Zone NO1\n",
    "RMSE 2023-24:  21.057\n",
    "\n",
    "Zone NO2\n",
    "RMSE 2023-24:  24.377\n",
    "\n",
    "Zone NO3\n",
    "RMSE 2023-24:  15.700\n",
    "\n",
    "Zone NO4\n",
    "RMSE 2023-24:  11.912\n",
    "\n",
    "Zone NO5\n",
    "RMSE 2023-24:  17.403"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd921814",
   "metadata": {},
   "source": [
    "# Deep MLP – Rolling‑window Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87dbbbc",
   "metadata": {},
   "source": [
    "## 1  Dataset & Split\n",
    "\n",
    "* **Zones:** NO1 – NO5 (one CSV per zone, columns identical).\n",
    "* **Rows:** one‑hour resolution aggregated to **one row = one day** by `build_regression_matrix`.\n",
    "* **Train interval:** **2019‑01‑01 – 2022‑12‑31**.\n",
    "* **Test interval:** **2023‑01‑01 – 2024‑12‑31** (730 days).\n",
    "* **Rolling window:** **D = 730 days** – for every test day the model re‑trains on the 730 days that *immediately precede* it.\n",
    "\n",
    "## 2  Feature Engineering\n",
    "\n",
    "### Per‑hour block (repeated 24×)\n",
    "\n",
    "```\n",
    "Price_s{h}               current‑day spot price (target)   \n",
    "Price_lag_{1,2,7}_s{h}   spot price lags (1,2,7 days)      \n",
    "{Load,Solar,WindOn,WindOff}_DA_lag_0_s{h}  day‑ahead quantities (no lag)  \n",
    "```\n",
    "\n",
    "*Total per‑hour columns: 8  →  24 × 8 = 192.*\n",
    "\n",
    "### Global block (shared by all hours)\n",
    "\n",
    "```\n",
    "WD_{1‑7}                       weekday dummies (Mon…Sun)   \n",
    "{Coal,NGas,Oil,EUA}_lag_2      fuel & EUA prices, 2‑day lag\n",
    "```\n",
    "\n",
    "*Columns: 7 + 4 = 11.*\n",
    "\n",
    "**Total feature width:** 192 + 11 = 203.\n",
    "\n",
    "## 3  Model Architecture – `DeepMLP`\n",
    "\n",
    "```\n",
    "Input  (203) ➜ Linear(203, **128**) ➜ LeakyReLU ➜ Dropout(p=0.10)\n",
    "            ➜ Linear(128, 128)      ➜ LeakyReLU ➜ Dropout(p=0.10)\n",
    "            ➜ Linear(128, **24**)   (one output per hour)\n",
    "```\n",
    "\n",
    "* **Depth:** 2 hidden layers (`n_hidden = 2`).\n",
    "* **Activation:** LeakyReLU (slope = 0.01, PyTorch default).\n",
    "* **Dropout:** 10 % after each activation.\n",
    "* **Parameter count:** `(203×128+128) + (128×128+128) + (128×24+24)` ≈ **54 k**.\n",
    "\n",
    "## 4  Training Loop (executed *per test day*)\n",
    "\n",
    "| step                  | detail                                                                            |\n",
    "| --------------------- | --------------------------------------------------------------------------------- |\n",
    "| **Standardise X & y** | z‑score using the current 730‑day window (mean/std computed *within* the window). |\n",
    "| **Optimizer**         | `torch.optim.Adam(lr = 1e‑3, weight_decay = 1e‑3)`                                |\n",
    "| **Batch size**        | 32 (full‑batch ≈ 730 rows also viable)                                            |\n",
    "| **Epochs**            | 60                                                                                |\n",
    "| **Loss**              | Mean‑squared error on the 24‑dim vector                                           |\n",
    "| **Prediction**        | after training, predict the *next* day, then un‑standardise                       |\n",
    "\n",
    "## 5  Evaluation Metric\n",
    "\n",
    "* **RMSE (NOK/MWh)** averaged over the 730 test days × 24 hours.\n",
    "* 2023‑24 scores observed so far:\n",
    "\n",
    "  * NO1 21.06\n",
    "  * NO2 24.38\n",
    "  * NO3 15.70\n",
    "  * NO4 11.91\n",
    "  * NO5 17.40\n",
    "    → **Average 18.10** (baseline LightGBM ≈ 8–9).\n",
    "\n",
    "## 6  Next planned extensions\n",
    "\n",
    "1. **Hour‑specific output heads** (shared trunk, 24 linear tails).\n",
    "2. **Learning‑rate schedule** (cosine decay).\n",
    "3. Hyper‑parameter sweep (`hidden_dim`, dropout, weight‑decay) with Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6cdf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Zone NO1 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dl-energy-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# global constants to all zones\n",
    "INIT_DATE_EXPERIMENTS = '2019-01-01'\n",
    "INIT_TEST_DATE        = '2023-01-01'\n",
    "FINAL_DATE_EXPERIMENTS= '2024-12-31'\n",
    "\n",
    "# hyperparameters\n",
    "WINDOW_DAYS   = 730                 \n",
    "HIDDEN_DIM    = 128 # initial - 50                  \n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY  = 1e-3\n",
    "EPOCHS        = 60\n",
    "BATCH_SIZE    = 256\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "repo_root  = Path.cwd().parents[1]\n",
    "mapcodes   = [\"NO1\", \"NO2\", \"NO3\", \"NO4\", \"NO5\"]\n",
    "zone_data  = {}          \n",
    "\n",
    "for code in mapcodes:\n",
    "    csv_path = repo_root / \"data\" / f\"data_{code}.csv\"\n",
    "    df_raw   = pd.read_csv(csv_path, parse_dates=[\"time_utc\"])\n",
    "\n",
    "    data_t, train_t, train_dates, price_t = prepare_dataset_tensor_modified(\n",
    "        csv_path,\n",
    "        tz      = \"CET\",\n",
    "        seed    = 42,\n",
    "        test_days = (pd.Timestamp(FINAL_DATE_EXPERIMENTS)\n",
    "                     - pd.Timestamp(INIT_TEST_DATE)).days + 1,\n",
    "        dtype   = torch.float64,\n",
    "    )\n",
    "\n",
    "    idx = pd.DatetimeIndex(sorted(train_dates))\n",
    "    start_i = idx.get_loc(pd.Timestamp(INIT_DATE_EXPERIMENTS))\n",
    "    end_i   = idx.get_loc(pd.Timestamp(FINAL_DATE_EXPERIMENTS))\n",
    "    data_t  = data_t[start_i:end_i+1]             \n",
    "    dates_t = pd.Series(train_dates[start_i:end_i+1])\n",
    "\n",
    "    zone_data[code] = dict(\n",
    "        df        = df_raw,\n",
    "        tensor    = data_t,\n",
    "        dates     = dates_t,\n",
    "        price_t   = price_t[start_i:end_i+1],\n",
    "    )\n",
    "\n",
    "# rolling-window MLP per zone\n",
    "rmse_dmlp_by_zone   = {}\n",
    "preds_dmlp_by_zone  = {}\n",
    "\n",
    "for code in mapcodes:\n",
    "    print(f\"\\n==== Zone {code} ====\")\n",
    "\n",
    "    reg_data = build_regression_matrix(\n",
    "        dat_eval = zone_data[code][\"tensor\"].cpu().numpy(),\n",
    "        days_eval= pd.to_datetime(zone_data[code][\"dates\"]),\n",
    "        reg_names= zone_data[code][\"df\"].columns[1:],   \n",
    "    )\n",
    "    reg_df   = reg_data[\"regmat\"].dropna().reset_index(drop=True)\n",
    "    dep_idx  = reg_data[\"dep_indices\"]\n",
    "\n",
    "    all_dates = pd.DatetimeIndex(sorted(zone_data[code][\"dates\"]\\\n",
    "                                        .iloc[len(zone_data[code][\"dates\"])\n",
    "                                             - len(reg_df):]))\n",
    "    test_start_row = all_dates.get_loc(pd.Timestamp(INIT_TEST_DATE))\n",
    "    test_end_row   = all_dates.get_loc(pd.Timestamp(FINAL_DATE_EXPERIMENTS))\n",
    "    horizon        = test_end_row - test_start_row + 1      \n",
    "\n",
    "    preds, trues = build_mlp_rolling_forecasts(\n",
    "        regmat_df   = reg_df.astype(\"float32\"),\n",
    "        dep_indices = dep_idx,\n",
    "        window      = WINDOW_DAYS,\n",
    "        horizon     = horizon,\n",
    "        start_row   = test_start_row,\n",
    "        hidden_dim  = HIDDEN_DIM,\n",
    "        lr          = LEARNING_RATE,\n",
    "        weight_decay= WEIGHT_DECAY,\n",
    "        batch_size  = BATCH_SIZE,\n",
    "        epochs      = EPOCHS,\n",
    "        device      = device,\n",
    "    )\n",
    "\n",
    "    rmse_dmlp = torch.sqrt(((preds - trues) ** 2).mean()).item()\n",
    "    rmse_dmlp_by_zone[code]  = rmse_dmlp\n",
    "    preds_dmlp_by_zone[code] = preds                   \n",
    "\n",
    "    print(f\"RMSE 2023-24: {rmse_dmlp:7.3f}\")\n",
    "\n",
    "print(\"\\n Rolling-MLP RMSE\")\n",
    "for z, r in rmse_dmlp_by_zone.items():\n",
    "    print(f\"{z}:  {r:7.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90da3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "# Try this only in Collab:\n",
    "#------------------------------\n",
    "\n",
    "# build one big training DataLoader spanning entire training window\n",
    "full_loader = DataLoader(\n",
    "    TensorDataset(\n",
    "        regmat[: train_end],\n",
    "        dep_var[: train_end]\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "# instantiate & train ONE model\n",
    "model = DeepMLP(...).to(device)\n",
    "opt   = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for xb, yb in full_loader:\n",
    "        opt.zero_grad()\n",
    "        loss_fn(model(xb), yb).backward()\n",
    "        opt.step()\n",
    "\n",
    "# roll forward window and predict all horizon in one go\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = []\n",
    "    for n in range(horizon):\n",
    "        idx = test_start + n\n",
    "        mean_x = regmat[idx-window:idx].mean(0, keepdim=True)\n",
    "        std_x  = regmat[idx-window:idx].std(0, keepdim=True).clamp_min(1)\n",
    "        x_test = ((regmat[idx:idx+1] - mean_x) / std_x)\n",
    "        pred_std = model(x_test)\n",
    "        mean_y = dep_var[idx-window:idx].mean(0, keepdim=True)\n",
    "        std_y  = dep_var[idx-window:idx].std(0, keepdim=True).clamp_min(1)\n",
    "        preds.append(pred_std * std_y + mean_y)\n",
    "    preds = torch.cat(preds, dim=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-energy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
