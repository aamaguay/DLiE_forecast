{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ba3accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1148c2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dl-energy-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %% load packages\n",
    "import locale\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import requests\n",
    "import torch\n",
    "import random\n",
    "from sqlalchemy import create_engine,inspect\n",
    "from pathlib import Path\n",
    "import urllib.parse\n",
    "import pyarrow\n",
    "from calendar import day_abbr\n",
    "import calendar\n",
    "from typing import Tuple, Union, Dict, List\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pygam import LinearGAM\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23d49975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from srs.utils.tutor_utils import prepare_dataset_tensor, forecasting_study,\\\n",
    "  plot_daily_profile,plot_hour_comparison, build_multiwindow_experts, tune_ewa_eta, \\\n",
    "  ewa_aggregate_forecasts, compute_error_table, tune_expert_window, \\\n",
    "  run_expert_window_test, build_regression_matrix, SimpleMLP, train_mlp, \\\n",
    "  prepare_train_test_tensors, build_mlp_rolling_forecasts, tune_mlp_hyperparameters, \\\n",
    "  DST_trafo\n",
    "\n",
    "from srs.utils.our_utils import run_forecast_step\n",
    "from srs.collect_data.setup import setup_seed, get_device\n",
    "from srs.collect_data.entsoe_data import create_entsoe_engine, get_tables, get_spec, \\\n",
    "  get_market_divisions,get_map_codes,get_map_codes_starting_with, get_resolution_codes, \\\n",
    "    prepare_generation, prepare_load,prepare_price, prepare_unavailability, \\\n",
    "    prepare_filling_rate_hydro, prepare_physical_flow, prepare_installed_capacity\n",
    "from srs.collect_data.datastream_data import create_datastream_engine, get_tables, \\\n",
    "  prepare_datastream\n",
    "from srs.collect_data.dwd_mosmix_data import fetch_region_weather, prepare_weather\n",
    "from srs.collect_data.merge_data import merge_datasets, build_training_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdadeae",
   "metadata": {},
   "source": [
    "### gam_24h and gam_1h fitting for no1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd91127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform merged dataset using DST_trafo and prepare training data.\n",
    "\n",
    "repo_root = Path.cwd().parents[1]\n",
    "data_no1 = pd.read_csv(repo_root / \"data\" /'data_no1.csv')\n",
    "data_t_no1, train_t_no1, train_dates, price_t_no1 = prepare_dataset_tensor(\n",
    "    repo_root / \"data\" / \"data_no1.csv\",\n",
    "    tz=\"CET\",\n",
    "    seed=42,\n",
    "    test_days=2*365,         \n",
    "    dtype=torch.float64, \n",
    ")\n",
    "\n",
    "data_array = data_t_no1         \n",
    "price_S    = price_t_no1        \n",
    "dates_S    = train_dates    \n",
    "\n",
    "train_start_idx = dates_idx.get_loc(pd.Timestamp(\"2019-01-01\"))\n",
    "train_end_idx   = dates_idx.get_loc(pd.Timestamp(\"2023-12-31\"))\n",
    "\n",
    "\n",
    "D          = 730            \n",
    "S          = 24\n",
    "WD         = [1, 6, 7]\n",
    "PRICE_S_LAGS = [1, 2, 7]\n",
    "da_lag = [0]\n",
    "\n",
    "#validation period length\n",
    "length_eval = 2 * 365\n",
    "\n",
    "# The first obdervation in the evaluation period\n",
    "begin_eval = data_array.shape[0] - length_eval\n",
    "\n",
    "N_s = length_eval\n",
    "\n",
    "model_names = [\n",
    "    \"true\",\n",
    "    \"expert_ext\",\n",
    "    \"linar_gam\",\n",
    "    \"light_gbm\"\n",
    "]\n",
    "n_models = len(model_names)\n",
    "\n",
    "# 3D tensor to hold forecasts:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "forecasts = torch.full((N_s, S, n_models), float('nan'), dtype=torch.float64, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b06b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create thread pool\n",
    "init_time = datetime.now()\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(\n",
    "            run_forecast_step,\n",
    "            n,\n",
    "            price_S,\n",
    "            data_array,\n",
    "            begin_eval,\n",
    "            D,\n",
    "            dates_S,\n",
    "            WD,\n",
    "            PRICE_S_LAGS,\n",
    "            da_lag,\n",
    "            data_no1.columns[1:],  # reg_names\n",
    "            data_no1.columns[1:]   # data_columns\n",
    "        )\n",
    "        for n in range(N_s)\n",
    "    ]\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            n, gam_24h, gam_per_hour = future.result()\n",
    "            forecasts[n, :, 0] = torch.tensor(gam_24h, dtype=forecasts.dtype, device=forecasts.device)\n",
    "            forecasts[n, :, 1] = torch.tensor(gam_per_hour, dtype=forecasts.dtype, device=forecasts.device)\n",
    "            #forecasts[n, :, insert_order] = true_price\n",
    "            #forecasts[n, :, insert_order] = torch.tensor(expert, dtype=forecasts.dtype, device=forecasts.device)\n",
    "            #forecasts[n, :, insert_order] = torch.tensor(lg_gbm, dtype=forecasts.dtype, device=forecasts.device)\n",
    "        except Exception as e:\n",
    "            print(f\"Thread crashed: {e}\")\n",
    "\n",
    "# End timing\n",
    "end_time = datetime.now()\n",
    "duration_minutes = (end_time - init_time).total_seconds() / 60\n",
    "print(f\"\\nParallel training duration (threaded): {duration_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7597c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save forecasts (v1 = version 1, add up as number of experiments increases)\n",
    "fc = forecasts.cpu().numpy()\n",
    "N_s, S, n_models = fc.shape\n",
    "\n",
    "samples = np.repeat(np.arange(N_s), S)\n",
    "hours = np.tile(np.arange(S), N_s)\n",
    "data = {\n",
    "    \"sample\": samples,\n",
    "    \"hours\": hours\n",
    "}\n",
    "\n",
    "for name, m in [(\"gam_24h\", 0), (\"gam_1h\", 1)]:\n",
    "    data[name] = fc[:,:,m].reshape(-1)\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(repo_root/\"data\"/\"forecasts_gam24h_gam_1h_v1.csv\", index=False)\n",
    "print(f\"Saved forecasts with columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5221d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_array.shape )\n",
    "# print(price_S.shape )\n",
    "# print(dates_S.shape )\n",
    "\n",
    "# print(data_t_no1.shape)\n",
    "# print(price_t_no1.shape)\n",
    "# print(train_dates.shape)\n",
    "# print(train_t_no1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9e3c1d",
   "metadata": {},
   "source": [
    "### gam_24h fitting for all no1-no5 regions separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af66cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  training interval:\n",
    "  2019 - 365 days\n",
    "  2020 - 366 days\n",
    "  2021 - 365 days\n",
    "  2022 - 366 days\n",
    "  \n",
    "  testing interval:\n",
    "  2023 - 365 days\n",
    "  2024 - 366 days\n",
    "  \n",
    "  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62157039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# **************************************\n",
    "# define dates for training and evaluation \n",
    "# **************************************\n",
    "INIT_DATE_EXPERIMENTS = '2019-01-01'\n",
    "INIT_TEST_DATE = '2023-01-01'\n",
    "FINAL_DATE_EXPERIMENTS = '2024-12-31'\n",
    "n_days_test = (pd.to_datetime(FINAL_DATE_EXPERIMENTS) - pd.to_datetime(INIT_TEST_DATE)).days + (1) # additional adjustment\n",
    "\n",
    "repo_root = Path.cwd().parents[1]\n",
    "mapcodes = [\"NO1\",\"NO2\",\"NO3\",\"NO4\",\"NO5\"]\n",
    "maps_dict = {}\n",
    "\n",
    "for code in mapcodes:\n",
    "    csv_path = repo_root / \"data\" / f\"data_{code}.csv\"\n",
    "    df = pd.read_csv(csv_path, parse_dates=[\"time_utc\"])\n",
    "    data_t, train_t, train_dates, price_t = prepare_dataset_tensor( # <- update function to Alex's one\n",
    "        csv_path,\n",
    "        tz=\"CET\",\n",
    "        seed=42,\n",
    "        test_days=n_days_test,\n",
    "        dtype=torch.float64,\n",
    "    )\n",
    "    \n",
    "    # fix potential problems with dates after change time zone.. (Alex correction)\n",
    "    train_dates_series       = pd.DatetimeIndex(sorted(train_dates))\n",
    "    id_init_exp = train_dates_series.get_loc(pd.Timestamp(INIT_DATE_EXPERIMENTS))\n",
    "    id_init_test_period = train_dates_series.get_loc(pd.Timestamp(INIT_TEST_DATE))\n",
    "    id_end_exp = train_dates_series.get_loc(pd.Timestamp(FINAL_DATE_EXPERIMENTS))\n",
    "    data_t = data_t[id_init_exp:(id_end_exp+1), :,:]\n",
    "    train_dates = pd.Series(train_dates[id_init_exp:(id_end_exp+1)])\n",
    "    \n",
    "    maps_dict[code] = {\n",
    "        \"df\": df,\n",
    "        \"data_t\": data_t,\n",
    "        \"train_t\": train_t,\n",
    "        \"train_dates\": train_dates,\n",
    "        \"price_t\": price_t\n",
    "    }\n",
    "maps_dict.keys()\n",
    "\n",
    "gam24_by_zone = {}\n",
    "rmse_by_zone  = {}\n",
    "\n",
    "for z in mapcodes:\n",
    "    print(f\"\\n--- {z} ---\")\n",
    "    price_S         = maps_dict[z][\"price_t\"]\n",
    "    data_array      = maps_dict[z][\"data_t\"]\n",
    "    full_dates      = maps_dict[z][\"train_dates\"] # <- changed from _all_ days to train_dates based on Alex spot\n",
    "    feature_names   = maps_dict[z][\"df\"].columns[1:]\n",
    "    full_date_series= pd.DatetimeIndex(sorted(full_dates)) \n",
    "\n",
    "    # evaluation days (all of 2024)\n",
    "    train_start_idx = full_date_series.get_loc(pd.Timestamp(INIT_DATE_EXPERIMENTS))\n",
    "    id_init_eval = full_date_series.get_loc(pd.Timestamp(INIT_TEST_DATE))\n",
    "    id_end_eval = full_date_series.get_loc(pd.Timestamp(FINAL_DATE_EXPERIMENTS))\n",
    "    eval_start_idx = id_init_eval \n",
    "    eval_end_idx  = id_end_eval\n",
    "    N_s = (eval_end_idx - eval_start_idx) + 1\n",
    "    full_dates = pd.to_datetime(full_dates)\n",
    "    \n",
    "    # new features: WD - dummy for week days, price lags for Mon, Tue and Fri, day-ahead load lag\n",
    "    WD             = [1,6,7]     \n",
    "    PRICE_S_LAGS   = [1,2,7]\n",
    "    DA_LAG         = [0]\n",
    "    S              = 24\n",
    "    #D             = 730\n",
    "\n",
    "    # tensors to collect forecasts for THIS zone\n",
    "    forecasts_zone = torch.full((N_s, S, 1), float(\"nan\"),\n",
    "                                dtype=torch.float64, device=device)\n",
    "\n",
    "    # thread pool\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                run_forecast_step_modified,\n",
    "                n,\n",
    "                price_S,\n",
    "                data_array,\n",
    "                train_start_idx = train_start_idx,\n",
    "                train_end_idx   = id_init_eval - 1,\n",
    "                full_dates      = full_dates,\n",
    "                wd              = WD,                \n",
    "                price_s_lags    = PRICE_S_LAGS,\n",
    "                da_lag          = DA_LAG,\n",
    "                feature_names   = feature_names,   # reg_names\n",
    "                n_trials_lgbm   = 10,\n",
    "            )\n",
    "            for n in range(N_s)\n",
    "        ]\n",
    "        for fut in as_completed(futures):\n",
    "            try:\n",
    "                n, gam24 = fut.result()\n",
    "                forecasts_zone[n, :, 0] = torch.tensor(gam24, dtype=forecasts_zone.dtype, device=device)\n",
    "            except Exception as e:\n",
    "                print(f\"Thread crashed: {e}\")\n",
    "                \n",
    "    #   shape: (N_s, S)\n",
    "    true_vals = price_S[eval_start_idx : eval_end_idx + 1].to(device)  \n",
    "    \n",
    "    # compute RMSE\n",
    "    diff = forecasts_zone[:, :, 0] - true_vals\n",
    "    rmse = torch.sqrt((diff**2).mean()).item()\n",
    "    \n",
    "    print(range(N_s))\n",
    "    print(f\"Zone {z} GAM-24h RMSE: {rmse:.4f}\")\n",
    "\n",
    "    gam24_by_zone[z] = forecasts_zone[:, :, 0].cpu()\n",
    "    rmse_by_zone[z]  = rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "babfb043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NO1 ---\n",
      "\n",
      "--- NO2 ---\n",
      "\n",
      "--- NO3 ---\n",
      "\n",
      "--- NO4 ---\n",
      "\n",
      "--- NO5 ---\n"
     ]
    }
   ],
   "source": [
    "#set the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "repo_root = Path.cwd().parents[1]\n",
    "mapcodes = [\"NO1\",\"NO2\",\"NO3\",\"NO4\",\"NO5\"]\n",
    "maps_dict = {}\n",
    "\n",
    "for code in mapcodes:\n",
    "    csv_path = repo_root / \"data\" / f\"data_{code}.csv\"\n",
    "    df = pd.read_csv(csv_path, parse_dates=[\"time_utc\"])\n",
    "    data_t, train_t, train_dates, price_t = prepare_dataset_tensor(\n",
    "        csv_path,\n",
    "        tz=\"CET\",\n",
    "        seed=42,\n",
    "        test_days=2*365 + 1,\n",
    "        dtype=torch.float64,\n",
    "    )\n",
    "    \n",
    "    maps_dict[code] = {\n",
    "        \"df\": df,\n",
    "        \"data_t\": data_t,\n",
    "        \"train_t\": train_t,\n",
    "        \"train_dates\": train_dates,\n",
    "        \"price_t\": price_t\n",
    "    }\n",
    "maps_dict.keys()\n",
    "\n",
    "gam24_by_zone   = {}\n",
    "rmse_by_zone  = {}\n",
    "\n",
    "for z in mapcodes:\n",
    "    print(f\"\\n--- {z} ---\")\n",
    "    #time_utc     = pd.to_datetime(maps_dict[z][\"df\"], utc=True)\n",
    "    time_utc_2   = maps_dict[z][\"df\"][\"time_utc\"].dt.normalize().unique() # <- added _all_ days\n",
    "    time_lt      = time_utc_2.tz_localize(\"CET\")\n",
    "    price_S      = maps_dict[z][\"price_t\"]\n",
    "    data_array   = maps_dict[z][\"data_t\"]\n",
    "    dates_S      = maps_dict[z][\"train_dates\"]\n",
    "    feature_names= maps_dict[z][\"df\"].columns[1:]\n",
    "    #data_columns = reg_names\n",
    "\n",
    "    # build a DatetimeIndex to locate our anchor dates\n",
    "    #dt_index       = pd.DatetimeIndex(pd.to_datetime(dates_S)) # <- typo here, delete this line afterward\n",
    "    full_dates       = pd.DatetimeIndex(sorted(full_dates))\n",
    "    train_start_idx = full_dates.get_loc(pd.Timestamp(\"2019-01-01\"))\n",
    "    train_end_idx   = full_dates.get_loc(pd.Timestamp(\"2023-12-31\")) # <- typo here, was 2022.12.31, changed to 23.12.31\n",
    "\n",
    "    # evaluation days (all of 2024)\n",
    "    eval_start_idx = train_end_idx + 1\n",
    "    eval_year = full_dates[eval_start_idx].year\n",
    "    eval_end_date = pd.Timestamp(f\"{eval_year}-12-31\")\n",
    "    eval_end_idx  = full_dates.get_loc(eval_end_date)\n",
    "    N_s = eval_end_idx - eval_start_idx + 1\n",
    "    \n",
    "    # new features: WD - dummy for week days, price lags for Mon, Tue and Fri, day-ahead load lag\n",
    "    WD             = [1,6,7]     \n",
    "    PRICE_S_LAGS   = [1,2,7]\n",
    "    DA_LAG         = [0]\n",
    "    S              = 24\n",
    "    #D             = 730\n",
    "\n",
    "    # tensors to collect forecasts for THIS zone\n",
    "    forecasts_zone = torch.full((N_s, S, 1), float(\"nan\"),\n",
    "                                dtype=torch.float64, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76fe1c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2191"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some = train_end_idx - train_start_idx\n",
    "some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aa1892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1462, 24, 10])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_t.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e1732fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1462,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16ee8df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2193, 24, 10])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5258e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2192"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e34f1810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "time_utc",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        }
       ],
       "ref": "4d4a76e3-b99a-4df0-a08c-cef1ddd40fc5",
       "rows": [
        [
         "0",
         "2019-01-01 00:00:00"
        ],
        [
         "1",
         "2019-01-01 00:00:00"
        ],
        [
         "2",
         "2019-01-01 00:00:00"
        ],
        [
         "3",
         "2019-01-01 00:00:00"
        ],
        [
         "4",
         "2019-01-01 00:00:00"
        ],
        [
         "5",
         "2019-01-01 00:00:00"
        ],
        [
         "6",
         "2019-01-01 00:00:00"
        ],
        [
         "7",
         "2019-01-01 00:00:00"
        ],
        [
         "8",
         "2019-01-01 00:00:00"
        ],
        [
         "9",
         "2019-01-01 00:00:00"
        ],
        [
         "10",
         "2019-01-01 00:00:00"
        ],
        [
         "11",
         "2019-01-01 00:00:00"
        ],
        [
         "12",
         "2019-01-01 00:00:00"
        ],
        [
         "13",
         "2019-01-01 00:00:00"
        ],
        [
         "14",
         "2019-01-01 00:00:00"
        ],
        [
         "15",
         "2019-01-01 00:00:00"
        ],
        [
         "16",
         "2019-01-01 00:00:00"
        ],
        [
         "17",
         "2019-01-01 00:00:00"
        ],
        [
         "18",
         "2019-01-01 00:00:00"
        ],
        [
         "19",
         "2019-01-01 00:00:00"
        ],
        [
         "20",
         "2019-01-01 00:00:00"
        ],
        [
         "21",
         "2019-01-01 00:00:00"
        ],
        [
         "22",
         "2019-01-01 00:00:00"
        ],
        [
         "23",
         "2019-01-01 00:00:00"
        ],
        [
         "24",
         "2019-01-02 00:00:00"
        ],
        [
         "25",
         "2019-01-02 00:00:00"
        ],
        [
         "26",
         "2019-01-02 00:00:00"
        ],
        [
         "27",
         "2019-01-02 00:00:00"
        ],
        [
         "28",
         "2019-01-02 00:00:00"
        ],
        [
         "29",
         "2019-01-02 00:00:00"
        ],
        [
         "30",
         "2019-01-02 00:00:00"
        ],
        [
         "31",
         "2019-01-02 00:00:00"
        ],
        [
         "32",
         "2019-01-02 00:00:00"
        ],
        [
         "33",
         "2019-01-02 00:00:00"
        ],
        [
         "34",
         "2019-01-02 00:00:00"
        ],
        [
         "35",
         "2019-01-02 00:00:00"
        ],
        [
         "36",
         "2019-01-02 00:00:00"
        ],
        [
         "37",
         "2019-01-02 00:00:00"
        ],
        [
         "38",
         "2019-01-02 00:00:00"
        ],
        [
         "39",
         "2019-01-02 00:00:00"
        ],
        [
         "40",
         "2019-01-02 00:00:00"
        ],
        [
         "41",
         "2019-01-02 00:00:00"
        ],
        [
         "42",
         "2019-01-02 00:00:00"
        ],
        [
         "43",
         "2019-01-02 00:00:00"
        ],
        [
         "44",
         "2019-01-02 00:00:00"
        ],
        [
         "45",
         "2019-01-02 00:00:00"
        ],
        [
         "46",
         "2019-01-02 00:00:00"
        ],
        [
         "47",
         "2019-01-02 00:00:00"
        ],
        [
         "48",
         "2019-01-03 00:00:00"
        ],
        [
         "49",
         "2019-01-03 00:00:00"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 52608
       }
      },
      "text/plain": [
       "0       2019-01-01\n",
       "1       2019-01-01\n",
       "2       2019-01-01\n",
       "3       2019-01-01\n",
       "4       2019-01-01\n",
       "           ...    \n",
       "52603   2024-12-31\n",
       "52604   2024-12-31\n",
       "52605   2024-12-31\n",
       "52606   2024-12-31\n",
       "52607   2024-12-31\n",
       "Name: time_utc, Length: 52608, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maps_dict[z][\"df\"][\"time_utc\"].dt.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04c1d51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "time_utc",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        }
       ],
       "ref": "87a13917-d33a-492d-8ac2-35c31cd32c61",
       "rows": [
        [
         "0",
         "2019-01-01 00:00:00"
        ],
        [
         "1",
         "2019-01-01 01:00:00"
        ],
        [
         "2",
         "2019-01-01 02:00:00"
        ],
        [
         "3",
         "2019-01-01 03:00:00"
        ],
        [
         "4",
         "2019-01-01 04:00:00"
        ],
        [
         "5",
         "2019-01-01 05:00:00"
        ],
        [
         "6",
         "2019-01-01 06:00:00"
        ],
        [
         "7",
         "2019-01-01 07:00:00"
        ],
        [
         "8",
         "2019-01-01 08:00:00"
        ],
        [
         "9",
         "2019-01-01 09:00:00"
        ],
        [
         "10",
         "2019-01-01 10:00:00"
        ],
        [
         "11",
         "2019-01-01 11:00:00"
        ],
        [
         "12",
         "2019-01-01 12:00:00"
        ],
        [
         "13",
         "2019-01-01 13:00:00"
        ],
        [
         "14",
         "2019-01-01 14:00:00"
        ],
        [
         "15",
         "2019-01-01 15:00:00"
        ],
        [
         "16",
         "2019-01-01 16:00:00"
        ],
        [
         "17",
         "2019-01-01 17:00:00"
        ],
        [
         "18",
         "2019-01-01 18:00:00"
        ],
        [
         "19",
         "2019-01-01 19:00:00"
        ],
        [
         "20",
         "2019-01-01 20:00:00"
        ],
        [
         "21",
         "2019-01-01 21:00:00"
        ],
        [
         "22",
         "2019-01-01 22:00:00"
        ],
        [
         "23",
         "2019-01-01 23:00:00"
        ],
        [
         "24",
         "2019-01-02 00:00:00"
        ],
        [
         "25",
         "2019-01-02 01:00:00"
        ],
        [
         "26",
         "2019-01-02 02:00:00"
        ],
        [
         "27",
         "2019-01-02 03:00:00"
        ],
        [
         "28",
         "2019-01-02 04:00:00"
        ],
        [
         "29",
         "2019-01-02 05:00:00"
        ],
        [
         "30",
         "2019-01-02 06:00:00"
        ],
        [
         "31",
         "2019-01-02 07:00:00"
        ],
        [
         "32",
         "2019-01-02 08:00:00"
        ],
        [
         "33",
         "2019-01-02 09:00:00"
        ],
        [
         "34",
         "2019-01-02 10:00:00"
        ],
        [
         "35",
         "2019-01-02 11:00:00"
        ],
        [
         "36",
         "2019-01-02 12:00:00"
        ],
        [
         "37",
         "2019-01-02 13:00:00"
        ],
        [
         "38",
         "2019-01-02 14:00:00"
        ],
        [
         "39",
         "2019-01-02 15:00:00"
        ],
        [
         "40",
         "2019-01-02 16:00:00"
        ],
        [
         "41",
         "2019-01-02 17:00:00"
        ],
        [
         "42",
         "2019-01-02 18:00:00"
        ],
        [
         "43",
         "2019-01-02 19:00:00"
        ],
        [
         "44",
         "2019-01-02 20:00:00"
        ],
        [
         "45",
         "2019-01-02 21:00:00"
        ],
        [
         "46",
         "2019-01-02 22:00:00"
        ],
        [
         "47",
         "2019-01-02 23:00:00"
        ],
        [
         "48",
         "2019-01-03 00:00:00"
        ],
        [
         "49",
         "2019-01-03 01:00:00"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 52608
       }
      },
      "text/plain": [
       "0       2019-01-01 00:00:00\n",
       "1       2019-01-01 01:00:00\n",
       "2       2019-01-01 02:00:00\n",
       "3       2019-01-01 03:00:00\n",
       "4       2019-01-01 04:00:00\n",
       "                ...        \n",
       "52603   2024-12-31 19:00:00\n",
       "52604   2024-12-31 20:00:00\n",
       "52605   2024-12-31 21:00:00\n",
       "52606   2024-12-31 22:00:00\n",
       "52607   2024-12-31 23:00:00\n",
       "Name: time_utc, Length: 52608, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maps_dict[z][\"df\"][\"time_utc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b682c4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2193, 24, 10])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "290bdf51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2192,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_lt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef9fb57d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2192,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_utc_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8445b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-energy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
