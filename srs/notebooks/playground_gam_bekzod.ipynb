{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ba3accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1148c2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dl-energy-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %% load packages\n",
    "import locale\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import requests\n",
    "import torch\n",
    "import random\n",
    "from sqlalchemy import create_engine,inspect\n",
    "from pathlib import Path\n",
    "import urllib.parse\n",
    "import pyarrow\n",
    "from calendar import day_abbr\n",
    "import calendar\n",
    "from typing import Tuple, Union, Dict, List\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pygam import LinearGAM\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d49975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from srs.utils.tutor_utils import prepare_dataset_tensor, forecasting_study,\\\n",
    "  plot_daily_profile,plot_hour_comparison, build_multiwindow_experts, tune_ewa_eta, \\\n",
    "  ewa_aggregate_forecasts, compute_error_table, tune_expert_window, \\\n",
    "  run_expert_window_test, build_regression_matrix, SimpleMLP, train_mlp, \\\n",
    "  prepare_train_test_tensors, build_mlp_rolling_forecasts, tune_mlp_hyperparameters, \\\n",
    "  DST_trafo\n",
    "\n",
    "from srs.utils.our_utils import run_forecast_step\n",
    "from srs.collect_data.setup import setup_seed, get_device\n",
    "from srs.collect_data.entsoe_data import create_entsoe_engine, get_tables, get_spec, \\\n",
    "  get_market_divisions,get_map_codes,get_map_codes_starting_with, get_resolution_codes, \\\n",
    "    prepare_generation, prepare_load,prepare_price, prepare_unavailability, \\\n",
    "    prepare_filling_rate_hydro, prepare_physical_flow, prepare_installed_capacity\n",
    "from srs.collect_data.datastream_data import create_datastream_engine, get_tables, \\\n",
    "  prepare_datastream\n",
    "from srs.collect_data.dwd_mosmix_data import fetch_region_weather, prepare_weather\n",
    "from srs.collect_data.merge_data import merge_datasets, build_training_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdadeae",
   "metadata": {},
   "source": [
    "### gam_24h and gam_1h fitting for no1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd91127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform merged dataset using DST_trafo and prepare training data.\n",
    "\n",
    "repo_root = Path.cwd().parents[1]\n",
    "data_no1 = pd.read_csv(repo_root / \"data\" /'data_no1.csv')\n",
    "data_t_no1, train_t_no1, train_dates, price_t_no1 = prepare_dataset_tensor(\n",
    "    repo_root / \"data\" / \"data_no1.csv\",\n",
    "    tz=\"CET\",\n",
    "    seed=42,\n",
    "    test_days=2*365,         \n",
    "    dtype=torch.float64, \n",
    ")\n",
    "\n",
    "data_array = data_t_no1         \n",
    "price_S    = price_t_no1        \n",
    "dates_S    = train_dates    \n",
    "\n",
    "train_start_idx = dates_idx.get_loc(pd.Timestamp(\"2019-01-01\"))\n",
    "train_end_idx   = dates_idx.get_loc(pd.Timestamp(\"2023-12-31\"))\n",
    "\n",
    "\n",
    "D          = 730            \n",
    "S          = 24\n",
    "WD         = [1, 6, 7]\n",
    "PRICE_S_LAGS = [1, 2, 7]\n",
    "da_lag = [0]\n",
    "\n",
    "#validation period length\n",
    "length_eval = 2 * 365\n",
    "\n",
    "# The first obdervation in the evaluation period\n",
    "begin_eval = data_array.shape[0] - length_eval\n",
    "\n",
    "N_s = length_eval\n",
    "\n",
    "model_names = [\n",
    "    \"true\",\n",
    "    \"expert_ext\",\n",
    "    \"linar_gam\",\n",
    "    \"light_gbm\"\n",
    "]\n",
    "n_models = len(model_names)\n",
    "\n",
    "# 3D tensor to hold forecasts:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "forecasts = torch.full((N_s, S, n_models), float('nan'), dtype=torch.float64, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b06b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create thread pool\n",
    "init_time = datetime.now()\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(\n",
    "            run_forecast_step,\n",
    "            n,\n",
    "            price_S,\n",
    "            data_array,\n",
    "            begin_eval,\n",
    "            D,\n",
    "            dates_S,\n",
    "            WD,\n",
    "            PRICE_S_LAGS,\n",
    "            da_lag,\n",
    "            data_no1.columns[1:],  # reg_names\n",
    "            data_no1.columns[1:]   # data_columns\n",
    "        )\n",
    "        for n in range(N_s)\n",
    "    ]\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            n, gam_24h, gam_per_hour = future.result()\n",
    "            forecasts[n, :, 0] = torch.tensor(gam_24h, dtype=forecasts.dtype, device=forecasts.device)\n",
    "            forecasts[n, :, 1] = torch.tensor(gam_per_hour, dtype=forecasts.dtype, device=forecasts.device)\n",
    "            #forecasts[n, :, insert_order] = true_price\n",
    "            #forecasts[n, :, insert_order] = torch.tensor(expert, dtype=forecasts.dtype, device=forecasts.device)\n",
    "            #forecasts[n, :, insert_order] = torch.tensor(lg_gbm, dtype=forecasts.dtype, device=forecasts.device)\n",
    "        except Exception as e:\n",
    "            print(f\"Thread crashed: {e}\")\n",
    "\n",
    "# End timing\n",
    "end_time = datetime.now()\n",
    "duration_minutes = (end_time - init_time).total_seconds() / 60\n",
    "print(f\"\\nParallel training duration (threaded): {duration_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7597c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save forecasts (v1 = version 1, add up as number of experiments increases)\n",
    "fc = forecasts.cpu().numpy()\n",
    "N_s, S, n_models = fc.shape\n",
    "\n",
    "samples = np.repeat(np.arange(N_s), S)\n",
    "hours = np.tile(np.arange(S), N_s)\n",
    "data = {\n",
    "    \"sample\": samples,\n",
    "    \"hours\": hours\n",
    "}\n",
    "\n",
    "for name, m in [(\"gam_24h\", 0), (\"gam_1h\", 1)]:\n",
    "    data[name] = fc[:,:,m].reshape(-1)\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(repo_root/\"data\"/\"forecasts_gam24h_gam_1h_v1.csv\", index=False)\n",
    "print(f\"Saved forecasts with columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5221d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_array.shape )\n",
    "# print(price_S.shape )\n",
    "# print(dates_S.shape )\n",
    "\n",
    "# print(data_t_no1.shape)\n",
    "# print(price_t_no1.shape)\n",
    "# print(train_dates.shape)\n",
    "# print(train_t_no1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9e3c1d",
   "metadata": {},
   "source": [
    "### gam_24h fitting for all no1-no5 regions separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af66cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  training interval:\n",
    "  2019 - 365 days\n",
    "  2020 - 366 days\n",
    "  2021 - 365 days\n",
    "  2022 - 366 days\n",
    "  \n",
    "  testing interval:\n",
    "  2023 - 365 days\n",
    "  2024 - 366 days\n",
    "  \n",
    "  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62157039",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Timestamp('2024-12-31 00:00:00')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:609\u001b[39m, in \u001b[36mpandas._libs.index.DatetimeEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[39m, in \u001b[36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[39m, in \u001b[36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 1735603200000000000",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dl-energy-env/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._engine.get_loc(casted_key)\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:577\u001b[39m, in \u001b[36mpandas._libs.index.DatetimeEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:611\u001b[39m, in \u001b[36mpandas._libs.index.DatetimeEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: Timestamp('2024-12-31 00:00:00')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dl-energy-env/lib/python3.11/site-packages/pandas/core/indexes/datetimes.py:630\u001b[39m, in \u001b[36mDatetimeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Index.get_loc(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dl-energy-env/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: Timestamp('2024-12-31 00:00:00')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m id_init_exp = train_dates_series.get_loc(pd.Timestamp(INIT_DATE_EXPERIMENTS))\n\u001b[32m     30\u001b[39m id_init_test_period = train_dates_series.get_loc(pd.Timestamp(INIT_TEST_DATE))\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m id_end_exp = train_dates_series.get_loc(pd.Timestamp(FINAL_DATE_EXPERIMENTS))\n\u001b[32m     32\u001b[39m data_t = data_t[id_init_exp:(id_end_exp+\u001b[32m1\u001b[39m), :,:]\n\u001b[32m     33\u001b[39m train_dates = pd.Series(train_dates[id_init_exp:(id_end_exp+\u001b[32m1\u001b[39m)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dl-energy-env/lib/python3.11/site-packages/pandas/core/indexes/datetimes.py:632\u001b[39m, in \u001b[36mDatetimeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Index.get_loc(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(orig_key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: Timestamp('2024-12-31 00:00:00')"
     ]
    }
   ],
   "source": [
    "#set the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# **************************************\n",
    "# define dates for training and evaluation \n",
    "# **************************************\n",
    "INIT_DATE_EXPERIMENTS = '2019-01-01'\n",
    "INIT_TEST_DATE = '2023-01-01'\n",
    "FINAL_DATE_EXPERIMENTS = '2024-12-31'\n",
    "n_days_test = (pd.to_datetime(FINAL_DATE_EXPERIMENTS) - pd.to_datetime(INIT_TEST_DATE)).days + (1) # additional adjustment\n",
    "\n",
    "repo_root = Path.cwd().parents[1]\n",
    "mapcodes = [\"NO1\",\"NO2\",\"NO3\",\"NO4\",\"NO5\"]\n",
    "maps_dict = {}\n",
    "\n",
    "for code in mapcodes:\n",
    "    csv_path = repo_root / \"data\" / f\"data_{code}.csv\"\n",
    "    df = pd.read_csv(csv_path, parse_dates=[\"time_utc\"])\n",
    "    data_t, train_t, train_dates, price_t = prepare_dataset_tensor( # <- update function to Alex's one\n",
    "        csv_path,\n",
    "        tz=\"CET\",\n",
    "        seed=42,\n",
    "        test_days=n_days_test,\n",
    "        dtype=torch.float64,\n",
    "    )\n",
    "    \n",
    "    # fix potential problems with dates after change time zone.. (Alex correction)\n",
    "    train_dates_series       = pd.DatetimeIndex(sorted(train_dates))\n",
    "    id_init_exp = train_dates_series.get_loc(pd.Timestamp(INIT_DATE_EXPERIMENTS))\n",
    "    id_init_test_period = train_dates_series.get_loc(pd.Timestamp(INIT_TEST_DATE))\n",
    "    id_end_exp = train_dates_series.get_loc(pd.Timestamp(FINAL_DATE_EXPERIMENTS))\n",
    "    data_t = data_t[id_init_exp:(id_end_exp+1), :,:]\n",
    "    train_dates = pd.Series(train_dates[id_init_exp:(id_end_exp+1)])\n",
    "    \n",
    "    maps_dict[code] = {\n",
    "        \"df\": df,\n",
    "        \"data_t\": data_t,\n",
    "        \"train_t\": train_t,\n",
    "        \"train_dates\": train_dates,\n",
    "        \"price_t\": price_t\n",
    "    }\n",
    "maps_dict.keys()\n",
    "\n",
    "gam24_by_zone = {}\n",
    "rmse_by_zone  = {}\n",
    "\n",
    "for z in mapcodes:\n",
    "    print(f\"\\n--- {z} ---\")\n",
    "    price_S         = maps_dict[z][\"price_t\"]\n",
    "    data_array      = maps_dict[z][\"data_t\"]\n",
    "    full_dates      = maps_dict[z][\"train_dates\"] # <- changed from _all_ days to train_dates based on Alex spot\n",
    "    feature_names   = maps_dict[z][\"df\"].columns[1:]\n",
    "    full_date_series= pd.DatetimeIndex(sorted(full_dates)) \n",
    "\n",
    "    # evaluation days (all of 2024)\n",
    "    train_start_idx = full_date_series.get_loc(pd.Timestamp(INIT_DATE_EXPERIMENTS))\n",
    "    id_init_eval = full_date_series.get_loc(pd.Timestamp(INIT_TEST_DATE))\n",
    "    id_end_eval = full_date_series.get_loc(pd.Timestamp(FINAL_DATE_EXPERIMENTS))\n",
    "    eval_start_idx = id_init_eval \n",
    "    eval_end_idx  = id_end_eval\n",
    "    N_s = (eval_end_idx - eval_start_idx) + 1\n",
    "    full_dates = pd.to_datetime(full_dates)\n",
    "    \n",
    "    # new features: WD - dummy for week days, price lags for Mon, Tue and Fri, day-ahead load lag\n",
    "    WD             = [1,6,7]     \n",
    "    PRICE_S_LAGS   = [1,2,7]\n",
    "    DA_LAG         = [0]\n",
    "    S              = 24\n",
    "    #D             = 730\n",
    "\n",
    "    # tensors to collect forecasts for THIS zone\n",
    "    forecasts_zone = torch.full((N_s, S, 1), float(\"nan\"),\n",
    "                                dtype=torch.float64, device=device)\n",
    "\n",
    "    # thread pool\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                run_forecast_step_modified,\n",
    "                n,\n",
    "                price_S,\n",
    "                data_array,\n",
    "                train_start_idx = train_start_idx,\n",
    "                train_end_idx   = id_init_eval - 1,\n",
    "                full_dates      = full_dates,\n",
    "                wd              = WD,                \n",
    "                price_s_lags    = PRICE_S_LAGS,\n",
    "                da_lag          = DA_LAG,\n",
    "                feature_names   = feature_names,   # reg_names\n",
    "                n_trials_lgbm   = 10,\n",
    "            )\n",
    "            for n in range(N_s)\n",
    "        ]\n",
    "        for fut in as_completed(futures):\n",
    "            try:\n",
    "                n, gam24 = fut.result()\n",
    "                forecasts_zone[n, :, 0] = torch.tensor(gam24, dtype=forecasts_zone.dtype, device=device)\n",
    "            except Exception as e:\n",
    "                print(f\"Thread crashed: {e}\")\n",
    "                \n",
    "    #   shape: (N_s, S)\n",
    "    true_vals = price_S[eval_start_idx : eval_end_idx + 1].to(device)  \n",
    "    \n",
    "    # compute RMSE\n",
    "    diff = forecasts_zone[:, :, 0] - true_vals\n",
    "    rmse = torch.sqrt((diff**2).mean()).item()\n",
    "    \n",
    "    print(range(N_s))\n",
    "    print(f\"Zone {z} GAM-24h RMSE: {rmse:.4f}\")\n",
    "\n",
    "    gam24_by_zone[z] = forecasts_zone[:, :, 0].cpu()\n",
    "    rmse_by_zone[z]  = rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babfb043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NO1 ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'full_dates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     39\u001b[39m feature_names= maps_dict[z][\u001b[33m\"\u001b[39m\u001b[33mdf\u001b[39m\u001b[33m\"\u001b[39m].columns[\u001b[32m1\u001b[39m:]\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m#data_columns = reg_names\u001b[39;00m\n\u001b[32m     41\u001b[39m \n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# build a DatetimeIndex to locate our anchor dates\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m#dt_index       = pd.DatetimeIndex(pd.to_datetime(dates_S)) # <- typo here, delete this line afterward\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m full_dates       = pd.DatetimeIndex(\u001b[38;5;28msorted\u001b[39m(full_dates))\n\u001b[32m     45\u001b[39m train_start_idx = full_dates.get_loc(pd.Timestamp(\u001b[33m\"\u001b[39m\u001b[33m2019-01-01\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     46\u001b[39m train_end_idx   = full_dates.get_loc(pd.Timestamp(\u001b[33m\"\u001b[39m\u001b[33m2023-12-31\u001b[39m\u001b[33m\"\u001b[39m)) \u001b[38;5;66;03m# <- typo here, was 2022.12.31, changed to 23.12.31\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'full_dates' is not defined"
     ]
    }
   ],
   "source": [
    "#set the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "repo_root = Path.cwd().parents[1]\n",
    "mapcodes = [\"NO1\",\"NO2\",\"NO3\",\"NO4\",\"NO5\"]\n",
    "maps_dict = {}\n",
    "\n",
    "for code in mapcodes:\n",
    "    csv_path = repo_root / \"data\" / f\"data_{code}.csv\"\n",
    "    df = pd.read_csv(csv_path, parse_dates=[\"time_utc\"])\n",
    "    data_t, train_t, train_dates, price_t = prepare_dataset_tensor(\n",
    "        csv_path,\n",
    "        tz=\"CET\",\n",
    "        seed=42,\n",
    "        test_days=2*365 + 1,\n",
    "        dtype=torch.float64,\n",
    "    )\n",
    "    \n",
    "    maps_dict[code] = {\n",
    "        \"df\": df,\n",
    "        \"data_t\": data_t,\n",
    "        \"train_t\": train_t,\n",
    "        \"train_dates\": train_dates,\n",
    "        \"price_t\": price_t\n",
    "    }\n",
    "maps_dict.keys()\n",
    "\n",
    "gam24_by_zone   = {}\n",
    "rmse_by_zone  = {}\n",
    "\n",
    "for z in mapcodes:\n",
    "    print(f\"\\n--- {z} ---\")\n",
    "    #time_utc     = pd.to_datetime(maps_dict[z][\"df\"], utc=True)\n",
    "    time_utc_2   = maps_dict[z][\"df\"][\"time_utc\"].dt.normalize().unique() # <- added _all_ days\n",
    "    time_lt      = time_utc_2.tz_localize(\"CET\")\n",
    "    price_S      = maps_dict[z][\"price_t\"]\n",
    "    data_array   = maps_dict[z][\"data_t\"]\n",
    "    dates_S      = maps_dict[z][\"train_dates\"]\n",
    "    feature_names= maps_dict[z][\"df\"].columns[1:]\n",
    "    #data_columns = reg_names\n",
    "\n",
    "    # build a DatetimeIndex to locate our anchor dates\n",
    "    full_dates       = pd.DatetimeIndex(sorted(full_dates))\n",
    "    train_start_idx = full_dates.get_loc(pd.Timestamp(\"2019-01-01\"))\n",
    "    train_end_idx   = full_dates.get_loc(pd.Timestamp(\"2023-12-31\")) # <- typo here, was 2022.12.31, changed to 23.12.31\n",
    "\n",
    "    # evaluation days (all of 2024)\n",
    "    eval_start_idx = train_end_idx + 1\n",
    "    eval_year = full_dates[eval_start_idx].year\n",
    "    eval_end_date = pd.Timestamp(f\"{eval_year}-12-31\")\n",
    "    eval_end_idx  = full_dates.get_loc(eval_end_date)\n",
    "    N_s = eval_end_idx - eval_start_idx + 1\n",
    "    \n",
    "    # new features: WD - dummy for week days, price lags for Mon, Tue and Fri, day-ahead load lag\n",
    "    WD             = [1,6,7]     \n",
    "    PRICE_S_LAGS   = [1,2,7]\n",
    "    DA_LAG         = [0]\n",
    "    S              = 24\n",
    "    #D             = 730\n",
    "\n",
    "    # tensors to collect forecasts for THIS zone\n",
    "    forecasts_zone = torch.full((N_s, S, 1), float(\"nan\"),\n",
    "                                dtype=torch.float64, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bc6f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-energy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
