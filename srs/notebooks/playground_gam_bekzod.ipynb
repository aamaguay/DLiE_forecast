{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba3accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1148c2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dl-energy-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %% load packages\n",
    "import locale\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import requests\n",
    "import torch\n",
    "import random\n",
    "from sqlalchemy import create_engine,inspect\n",
    "from pathlib import Path\n",
    "import urllib.parse\n",
    "import pyarrow\n",
    "from calendar import day_abbr\n",
    "import calendar\n",
    "from typing import Tuple, Union, Dict, List\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pygam import LinearGAM\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d49975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from srs.utils.tutor_utils import prepare_dataset_tensor, forecasting_study,\\\n",
    "  plot_daily_profile,plot_hour_comparison, build_multiwindow_experts, tune_ewa_eta, \\\n",
    "  ewa_aggregate_forecasts, compute_error_table, tune_expert_window, \\\n",
    "  run_expert_window_test, build_regression_matrix, SimpleMLP, train_mlp, \\\n",
    "  prepare_train_test_tensors, build_mlp_rolling_forecasts, tune_mlp_hyperparameters, \\\n",
    "  DST_trafo\n",
    "\n",
    "from srs.utils.our_utils import run_forecast_step\n",
    "from srs.collect_data.setup import setup_seed, get_device\n",
    "from srs.collect_data.entsoe_data import create_entsoe_engine, get_tables, get_spec, \\\n",
    "  get_market_divisions,get_map_codes,get_map_codes_starting_with, get_resolution_codes, \\\n",
    "    prepare_generation, prepare_load,prepare_price, prepare_unavailability, \\\n",
    "    prepare_filling_rate_hydro, prepare_physical_flow, prepare_installed_capacity\n",
    "from srs.collect_data.datastream_data import create_datastream_engine, get_tables, \\\n",
    "  prepare_datastream\n",
    "from srs.collect_data.dwd_mosmix_data import fetch_region_weather, prepare_weather\n",
    "from srs.collect_data.merge_data import merge_datasets, build_training_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdadeae",
   "metadata": {},
   "source": [
    "### gam_24h and gam_1h fitting for no1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd91127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform merged dataset using DST_trafo and prepare training data.\n",
    "\n",
    "repo_root = Path.cwd().parents[1]\n",
    "data_no1 = pd.read_csv(repo_root / \"data\" /'data_no1.csv')\n",
    "data_t_no1, train_t_no1, train_dates, price_t_no1 = prepare_dataset_tensor(\n",
    "    repo_root / \"data\" / \"data_no1.csv\",\n",
    "    tz=\"CET\",\n",
    "    seed=42,\n",
    "    test_days=2*365,         \n",
    "    dtype=torch.float64, \n",
    ")\n",
    "\n",
    "data_array = data_t_no1         \n",
    "price_S    = price_t_no1        \n",
    "dates_S    = train_dates    \n",
    "\n",
    "train_start_idx = dates_idx.get_loc(pd.Timestamp(\"2019-01-01\"))\n",
    "train_end_idx   = dates_idx.get_loc(pd.Timestamp(\"2023-12-31\"))\n",
    "\n",
    "\n",
    "D          = 730            \n",
    "S          = 24\n",
    "WD         = [1, 6, 7]\n",
    "PRICE_S_LAGS = [1, 2, 7]\n",
    "da_lag = [0]\n",
    "\n",
    "#validation period length\n",
    "length_eval = 2 * 365\n",
    "\n",
    "# The first obdervation in the evaluation period\n",
    "begin_eval = data_array.shape[0] - length_eval\n",
    "\n",
    "N_s = length_eval\n",
    "\n",
    "model_names = [\n",
    "    \"true\",\n",
    "    \"expert_ext\",\n",
    "    \"linar_gam\",\n",
    "    \"light_gbm\"\n",
    "]\n",
    "n_models = len(model_names)\n",
    "\n",
    "# 3D tensor to hold forecasts:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "forecasts = torch.full((N_s, S, n_models), float('nan'), dtype=torch.float64, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b06b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create thread pool\n",
    "init_time = datetime.now()\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(\n",
    "            run_forecast_step,\n",
    "            n,\n",
    "            price_S,\n",
    "            data_array,\n",
    "            begin_eval,\n",
    "            D,\n",
    "            dates_S,\n",
    "            WD,\n",
    "            PRICE_S_LAGS,\n",
    "            da_lag,\n",
    "            data_no1.columns[1:],  # reg_names\n",
    "            data_no1.columns[1:]   # data_columns\n",
    "        )\n",
    "        for n in range(N_s)\n",
    "    ]\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            n, gam_24h, gam_per_hour = future.result()\n",
    "            forecasts[n, :, 0] = torch.tensor(gam_24h, dtype=forecasts.dtype, device=forecasts.device)\n",
    "            forecasts[n, :, 1] = torch.tensor(gam_per_hour, dtype=forecasts.dtype, device=forecasts.device)\n",
    "            #forecasts[n, :, insert_order] = true_price\n",
    "            #forecasts[n, :, insert_order] = torch.tensor(expert, dtype=forecasts.dtype, device=forecasts.device)\n",
    "            #forecasts[n, :, insert_order] = torch.tensor(lg_gbm, dtype=forecasts.dtype, device=forecasts.device)\n",
    "        except Exception as e:\n",
    "            print(f\"Thread crashed: {e}\")\n",
    "\n",
    "# End timing\n",
    "end_time = datetime.now()\n",
    "duration_minutes = (end_time - init_time).total_seconds() / 60\n",
    "print(f\"\\nParallel training duration (threaded): {duration_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7597c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save forecasts (v1 = version 1, add up as number of experiments increases)\n",
    "fc = forecasts.cpu().numpy()\n",
    "N_s, S, n_models = fc.shape\n",
    "\n",
    "samples = np.repeat(np.arange(N_s), S)\n",
    "hours = np.tile(np.arange(S), N_s)\n",
    "data = {\n",
    "    \"sample\": samples,\n",
    "    \"hours\": hours\n",
    "}\n",
    "\n",
    "for name, m in [(\"gam_24h\", 0), (\"gam_1h\", 1)]:\n",
    "    data[name] = fc[:,:,m].reshape(-1)\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(repo_root/\"data\"/\"forecasts_gam24h_gam_1h_v1.csv\", index=False)\n",
    "print(f\"Saved forecasts with columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5221d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_array.shape )\n",
    "# print(price_S.shape )\n",
    "# print(dates_S.shape )\n",
    "\n",
    "# print(data_t_no1.shape)\n",
    "# print(price_t_no1.shape)\n",
    "# print(train_dates.shape)\n",
    "# print(train_t_no1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9e3c1d",
   "metadata": {},
   "source": [
    "### gam_24h fitting for all no1-no5 regions separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af66cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  eval/test periods\n",
    "  2023 - 365 days\n",
    "  2024 - 366 days\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62157039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NO1 ---\n",
      "Loop   0: train 2019-01-01 00:00:00 -> 2023-12-31 00:00:00, forecast 2024-01-01 00:00:00\n",
      "Loop   1: train 2019-01-01 00:00:00 -> 2024-01-01 00:00:00, forecast 2024-01-02 00:00:00\n",
      "Loop   2: train 2019-01-01 00:00:00 -> 2024-01-02 00:00:00, forecast 2024-01-03 00:00:00\n",
      "Loop   3: train 2019-01-01 00:00:00 -> 2024-01-03 00:00:00, forecast 2024-01-04 00:00:00\n",
      "Loop   4: train 2019-01-01 00:00:00 -> 2024-01-04 00:00:00, forecast 2024-01-05 00:00:00\n",
      "Loop   5: train 2019-01-01 00:00:00 -> 2024-01-05 00:00:00, forecast 2024-01-06 00:00:00\n",
      "Loop   6: train 2019-01-01 00:00:00 -> 2024-01-06 00:00:00, forecast 2024-01-07 00:00:00\n",
      "Loop   7: train 2019-01-01 00:00:00 -> 2024-01-07 00:00:00, forecast 2024-01-08 00:00:00\n",
      "Loop   8: train 2019-01-01 00:00:00 -> 2024-01-08 00:00:00, forecast 2024-01-09 00:00:00\n",
      "Loop  10: train 2019-01-01 00:00:00 -> 2024-01-10 00:00:00, forecast 2024-01-11 00:00:00\n",
      "Loop   9: train 2019-01-01 00:00:00 -> 2024-01-09 00:00:00, forecast 2024-01-10 00:00:00\n",
      "Loop  11: train 2019-01-01 00:00:00 -> 2024-01-11 00:00:00, forecast 2024-01-12 00:00:00\n",
      "-> finished loop 2\n",
      "Loop  12: train 2019-01-01 00:00:00 -> 2024-01-12 00:00:00, forecast 2024-01-13 00:00:00\n",
      "-> finished loop 1\n",
      "Loop  13: train 2019-01-01 00:00:00 -> 2024-01-13 00:00:00, forecast 2024-01-14 00:00:00\n",
      "-> finished loop 0\n",
      "Loop  14: train 2019-01-01 00:00:00 -> 2024-01-14 00:00:00, forecast 2024-01-15 00:00:00\n",
      "-> finished loop 10\n",
      "Loop  15: train 2019-01-01 00:00:00 -> 2024-01-15 00:00:00, forecast 2024-01-16 00:00:00\n",
      "-> finished loop 5\n",
      "Loop  16: train 2019-01-01 00:00:00 -> 2024-01-16 00:00:00, forecast 2024-01-17 00:00:00\n",
      "-> finished loop 7\n",
      "Loop  17: train 2019-01-01 00:00:00 -> 2024-01-17 00:00:00, forecast 2024-01-18 00:00:00\n",
      "-> finished loop 3\n",
      "Loop  18: train 2019-01-01 00:00:00 -> 2024-01-18 00:00:00, forecast 2024-01-19 00:00:00\n",
      "-> finished loop 4\n",
      "Loop  19: train 2019-01-01 00:00:00 -> 2024-01-19 00:00:00, forecast 2024-01-20 00:00:00\n",
      "-> finished loop 6\n",
      "Loop  20: train 2019-01-01 00:00:00 -> 2024-01-20 00:00:00, forecast 2024-01-21 00:00:00\n",
      "-> finished loop 8\n",
      "Loop  21: train 2019-01-01 00:00:00 -> 2024-01-21 00:00:00, forecast 2024-01-22 00:00:00\n",
      "-> finished loop 11\n",
      "Loop  22: train 2019-01-01 00:00:00 -> 2024-01-22 00:00:00, forecast 2024-01-23 00:00:00\n",
      "-> finished loop 9\n",
      "Loop  23: train 2019-01-01 00:00:00 -> 2024-01-23 00:00:00, forecast 2024-01-24 00:00:00\n",
      "-> finished loop 13\n",
      "Loop  24: train 2019-01-01 00:00:00 -> 2024-01-24 00:00:00, forecast 2024-01-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#set the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "repo_root = Path.cwd().parents[1]\n",
    "mapcodes = [\"NO1\",\"NO2\",\"NO3\",\"NO4\",\"NO5\"]\n",
    "maps_dict = {}\n",
    "\n",
    "for code in mapcodes:\n",
    "    csv_path = repo_root / \"data\" / f\"data_{code}.csv\"\n",
    "    df = pd.read_csv(csv_path, parse_dates=[\"time_utc\"])\n",
    "    data_t, train_t, train_dates, price_t = prepare_dataset_tensor(\n",
    "        csv_path,\n",
    "        tz=\"CET\",\n",
    "        seed=42,\n",
    "        test_days=2*365 + 1,\n",
    "        dtype=torch.float64,\n",
    "    )\n",
    "    \n",
    "    maps_dict[code] = {\n",
    "        \"df\": df,\n",
    "        \"data_t\": data_t,\n",
    "        \"train_t\": train_t,\n",
    "        \"train_dates\": train_dates,\n",
    "        \"price_t\": price_t\n",
    "    }\n",
    "maps_dict.keys()\n",
    "\n",
    "gam24_by_zone   = {}\n",
    "rmse_by_zone  = {}\n",
    "\n",
    "for z in mapcodes:\n",
    "    print(f\"\\n--- {z} ---\")\n",
    "    full_dates   = maps_dict[z][\"df\"][\"time_utc\"].dt.normalize().unique() # <- added _all_ days\n",
    "    price_S      = maps_dict[z][\"price_t\"]\n",
    "    data_array   = maps_dict[z][\"data_t\"]\n",
    "    dates_S      = maps_dict[z][\"train_dates\"]\n",
    "    feature_names= maps_dict[z][\"df\"].columns[1:]\n",
    "    #data_columns = reg_names\n",
    "\n",
    "    # build a DatetimeIndex to locate our anchor dates\n",
    "    #dt_index       = pd.DatetimeIndex(pd.to_datetime(dates_S)) # <- typo here, delete this line afterward\n",
    "    full_dates       = pd.DatetimeIndex(sorted(full_dates))\n",
    "    train_start_idx = full_dates.get_loc(pd.Timestamp(\"2019-01-01\"))\n",
    "    train_end_idx   = full_dates.get_loc(pd.Timestamp(\"2023-12-31\")) # <- typo here, was 2022.12.31, changed to 23.12.31\n",
    "\n",
    "    # evaluation days (all of 2024)\n",
    "    eval_start_idx = train_end_idx + 1 \n",
    "    eval_year = full_dates[eval_start_idx].year\n",
    "    eval_end_date = pd.Timestamp(f\"{eval_year}-12-31\")\n",
    "    eval_end_idx  = full_dates.get_loc(eval_end_date)\n",
    "    N_s = eval_end_idx - eval_start_idx + 1\n",
    "    \n",
    "    # new features: WD - dummy for week days, price lags for Mon, Tue and Fri, day-ahead load lag\n",
    "    WD             = [1,6,7]     \n",
    "    PRICE_S_LAGS   = [1,2,7]\n",
    "    DA_LAG         = [0]\n",
    "    S              = 24\n",
    "    #D             = 730\n",
    "\n",
    "    # tensors to collect forecasts for THIS zone\n",
    "    forecasts_zone = torch.full((N_s, S, 1), float(\"nan\"),\n",
    "                                dtype=torch.float64, device=device)\n",
    "\n",
    "    # thread pool\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                run_forecast_step,\n",
    "                n,\n",
    "                price_S,\n",
    "                data_array,\n",
    "                train_start_idx,\n",
    "                train_end_idx,\n",
    "                full_dates,\n",
    "                WD,                \n",
    "                PRICE_S_LAGS,\n",
    "                DA_LAG,\n",
    "                feature_names,   # reg_names\n",
    "            )\n",
    "            for n in range(N_s)\n",
    "        ]\n",
    "        for fut in as_completed(futures):\n",
    "            try:\n",
    "                n, gam24 = fut.result()\n",
    "                forecasts_zone[n, :, 0] = torch.tensor(gam24, dtype=forecasts_zone.dtype, device=device)\n",
    "            except Exception as e:\n",
    "                print(f\"Thread crashed: {e}\")\n",
    "                \n",
    "    #   shape: (N_s, S)\n",
    "    true_vals = price_S[eval_start_idx : eval_end_idx + 1].to(device)  \n",
    "    \n",
    "    # compute RMSE\n",
    "    diff = forecasts_zone[:, :, 0] - true_vals\n",
    "    rmse = torch.sqrt((diff**2).mean()).item()\n",
    "    \n",
    "    print(range(N_s))\n",
    "    print(f\"Zone {z} GAM-24h RMSE: {rmse:.4f}\")\n",
    "\n",
    "    gam24_by_zone[z] = forecasts_zone[:, :, 0].cpu()\n",
    "    rmse_by_zone[z]  = rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babfb043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-energy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
